The operation `1.4142135623730951 * 2^64` isn't a direct bit shift of the floating-point representation.

转换过程 【直接相乘，去掉小数部分】 int(price * q64.64)
1. `1.4142135623730951` is a floating-point number with its own IEEE-754 binary representation (with sign bit, exponent, and mantissa)

2. When you multiply by `2^64`:
   - It's a regular floating-point multiplication operation
   - The result is still a floating-point number (approximately 2.6087635650665566e+19)
   - This is NOT the same as bit-shifting the original floating-point representation

3. When this result is cast to a `u128` integer:
   - The fractional part is truncated
   - You get the integer 26087635650665566208

The operation is conceptually similar to shifting the decimal point, but it's not a direct bit shift of the original binary representation. It's a mathematical multiplication followed by a conversion to integer.

If you were working with an integer to begin with, then yes, multiplying by 2^64 would be equivalent to left-shifting by 64 bits. But since you're starting with a floating-point number, it's a regular multiplication operation.



2. 小数部分被truncated了
When you cast a floating-point number to an integer in programming languages like Rust (which is what happens in the code with `as u128`), the fractional part is truncated (cut off), not rounded.

This happens because:

1. Integers can't store fractional values by definition
2. When converting from floating-point to integer, the language must decide what to do with the fractional part
3. Most languages, including Rust, simply discard the fractional part (truncate toward zero)

In your specific calculation:
- `1.4142135623730951 * 2^64` = approximately 26087635650665566208.something
- When cast to `u128`, it becomes exactly 26087635650665566208

The "something" (any remaining tiny fractional part) gets truncated. However, because you've already multiplied by such a large number (2^64), the fractional part that gets truncated is extremely small relative to the original value, so the precision loss is minimal.

This is why fixed-point arithmetic uses large scaling factors - to push the significant digits of the fractional part into the integer range before truncation happens, minimizing precision loss.


3. 精度丢失问题
Yes, converting from Q64.64 fixed-point to a floating-point value can cause precision loss, but for two different reasons:

1. **Initial truncation when creating the Q64.64 number**:
   - When you convert from float to Q64.64 (`price_to_x64`), any fractional bits beyond the 64 bits of precision are truncated
   - This is a one-time precision loss that happens during the initial conversion

2. **Float precision limitations when converting back**:
   - When converting from Q64.64 back to float (`from_x64_price`), you're limited by the precision of the float type
   - A standard f64 has about 53 bits of precision in its mantissa
   - Your Q64.64 number has 128 bits total, so information can be lost when converting to f64

For most practical values, the precision loss is minimal because:
- Most real-world values don't need all 64 bits for their fractional part
- The f64 type's 53 bits of precision is sufficient for many applications

However, if you need to represent numbers with extremely high precision, you might notice some loss when round-tripping through Q64.64 to f64 and back.

For example, very small fractional differences might be preserved in the Q64.64 representation but lost when converted to f64.

